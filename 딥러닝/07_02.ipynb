{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966b2a2e-369f-41aa-ad0f-0267eb87db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6eb9ccd-fdf2-4c9f-be88-ef36d3e8647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7013adba-99a8-451d-8ac2-9b99f2d6b2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 10s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet_model = ResNet50(input_shape=(224,224,3),include_top=False) #include_top : 출력층을 제외하고 모델을 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e4a1422-4999-4aee-a6c7-5d7227a547f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.trainable = True\n",
    "model = Sequential([\n",
    "    resnet_model,\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e22f5f-85d6-4941-9aa2-48260059f0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               12845184  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36433283 (138.98 MB)\n",
      "Trainable params: 36380163 (138.78 MB)\n",
      "Non-trainable params: 53120 (207.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24331ce7-d6ff-48f0-8148-75cbaa2308ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_gen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                               horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9522624-8e7d-4ed4-811e-2cf5b83538a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1394 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_gen.flow_from_directory('./glaucoma/train', target_size=(224,224), class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82ec05e7-7b10-4532-8660-f092b08f2c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator()\n",
    "test_data = test_gen.flow_from_directory('./glaucoma/test', target_size=(224,224), class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0912fa62-2734-4fdd-bdc5-172fa3aac240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c5f871d-cfb0-4d0f-8c98-10fe679f810f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "44/44 [==============================] - 228s 5s/step - loss: 4.3390 - accuracy: 0.6607 - val_loss: 28723.4336 - val_accuracy: 0.3267\n",
      "Epoch 2/20\n",
      "44/44 [==============================] - 209s 5s/step - loss: 0.6322 - accuracy: 0.7253 - val_loss: 1.7920 - val_accuracy: 0.6200\n",
      "Epoch 3/20\n",
      "44/44 [==============================] - 210s 5s/step - loss: 0.5764 - accuracy: 0.7403 - val_loss: 1.1752 - val_accuracy: 0.6133\n",
      "Epoch 4/20\n",
      "44/44 [==============================] - 209s 5s/step - loss: 0.5731 - accuracy: 0.7575 - val_loss: 1.0294 - val_accuracy: 0.7400\n",
      "Epoch 5/20\n",
      "44/44 [==============================] - 208s 5s/step - loss: 0.7321 - accuracy: 0.7496 - val_loss: 0.6760 - val_accuracy: 0.6933\n",
      "Epoch 6/20\n",
      "44/44 [==============================] - 212s 5s/step - loss: 0.5862 - accuracy: 0.7511 - val_loss: 0.7104 - val_accuracy: 0.6667\n",
      "Epoch 7/20\n",
      "44/44 [==============================] - 211s 5s/step - loss: 0.5367 - accuracy: 0.7719 - val_loss: 1.0710 - val_accuracy: 0.6600\n",
      "Epoch 8/20\n",
      "44/44 [==============================] - 229s 5s/step - loss: 0.5429 - accuracy: 0.7690 - val_loss: 0.8716 - val_accuracy: 0.6600\n",
      "Epoch 9/20\n",
      "44/44 [==============================] - 216s 5s/step - loss: 0.5326 - accuracy: 0.7755 - val_loss: 0.5982 - val_accuracy: 0.7333\n",
      "Epoch 10/20\n",
      "44/44 [==============================] - 217s 5s/step - loss: 0.5242 - accuracy: 0.7905 - val_loss: 3.1910 - val_accuracy: 0.6467\n",
      "Epoch 11/20\n",
      "44/44 [==============================] - 213s 5s/step - loss: 0.5036 - accuracy: 0.7812 - val_loss: 0.8294 - val_accuracy: 0.7200\n",
      "Epoch 12/20\n",
      "44/44 [==============================] - 214s 5s/step - loss: 0.5638 - accuracy: 0.7869 - val_loss: 0.5686 - val_accuracy: 0.7467\n",
      "Epoch 13/20\n",
      "44/44 [==============================] - 215s 5s/step - loss: 0.4748 - accuracy: 0.7956 - val_loss: 0.6622 - val_accuracy: 0.7467\n",
      "Epoch 14/20\n",
      "44/44 [==============================] - 212s 5s/step - loss: 0.4962 - accuracy: 0.7956 - val_loss: 0.7409 - val_accuracy: 0.7333\n",
      "Epoch 15/20\n",
      "44/44 [==============================] - 209s 5s/step - loss: 0.4755 - accuracy: 0.8020 - val_loss: 0.5732 - val_accuracy: 0.7467\n",
      "Epoch 16/20\n",
      "44/44 [==============================] - 212s 5s/step - loss: 0.4439 - accuracy: 0.8156 - val_loss: 0.8478 - val_accuracy: 0.7267\n",
      "Epoch 17/20\n",
      "44/44 [==============================] - 214s 5s/step - loss: 0.4445 - accuracy: 0.8135 - val_loss: 0.5929 - val_accuracy: 0.7467\n",
      "Epoch 18/20\n",
      "44/44 [==============================] - 216s 5s/step - loss: 0.4418 - accuracy: 0.8099 - val_loss: 1.3350 - val_accuracy: 0.6067\n",
      "Epoch 19/20\n",
      "44/44 [==============================] - 219s 5s/step - loss: 0.4380 - accuracy: 0.8242 - val_loss: 2.3595 - val_accuracy: 0.6067\n",
      "Epoch 20/20\n",
      "44/44 [==============================] - 218s 5s/step - loss: 0.4463 - accuracy: 0.8013 - val_loss: 1.4619 - val_accuracy: 0.6467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b335c6c690>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=test_data, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb157ecd-b911-444c-a1bf-f1de15bec25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 934ms/step\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(\"sample.png\", target_size=(224,224))\n",
    "x = image.img_to_array(img).reshape(-1, 224, 224, 3)\n",
    "pred = model.predict(x)\n",
    "print(np.argmax(pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04201aa-4cd8-4b14-91aa-b98fe5d4ad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.14",
   "language": "python",
   "name": "tf2.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
