{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868ad24f-6dac-4f02-aa48-498054d2a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17d0d3-9cb6-4adf-968c-279b3039c13e",
   "metadata": {},
   "source": [
    "사전 학습된 모델과 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cfa21b-41f0-4882-95d5-a135d55ba488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7fcf53284f4415aa14e13e2301c91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KOSA\\anaconda3\\envs\\tf2.14\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KOSA\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f969dbcedf44212b8d152d196ffc990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d9d68d54f14f328b925506b383cf21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bb58fa118347c4adaaf3772ddff2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dddba400a684ddd986d33f26341a669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f135cbdc834d84ae73ed9358a5e9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'left'\n",
    "model = TFAutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f9960-46a7-471a-b619-a1c0a544e62a",
   "metadata": {},
   "source": [
    "입력값으로 응답 생성 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7cfd754-c21f-423a-896f-bba486a659ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_ids = None\n",
    "def chat_with_bot(user_input, chat_history_ids=None):\n",
    "    #입력 문장 token화\n",
    "    new_user_input_ids = tokenizer.encode(user_input+tokenizer.eos_token, return_tensors='tf')\n",
    "    #이전 대화 히스토리와 새로운 입력 결합\n",
    "    bot_input_ids = tf.concat([chat_history_ids, new_user_input_ids], axis=-1) if chat_history_ids is not None else new_user_input_ids\n",
    "    #모델 사용하여 응답 생성\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id = tokenizer.eos_token_id)\n",
    "    #모델의 출력을 디코딩하여 응답 문장 생성\n",
    "    bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "    return bot_response, chat_history_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e85c9-324d-4784-8769-fc6bff8dbfe8",
   "metadata": {},
   "source": [
    "채팅 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4aa8c1d-20a3-49e0-9fcc-af34cb4bc365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot : Hello! What can i help you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You :  Hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot : Hello! :D\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You :  Nice to meet you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot : Nice to meet you too!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You :  I'm tired\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot : I'm tired too\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You :  WTF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot : I'm tired too\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You :  Good bye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot : Bye bye\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You :  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot : Good Bye!\n"
     ]
    }
   ],
   "source": [
    "print(\"ChatBot : Hello! What can i help you?\")\n",
    "while True:\n",
    "    user_input = input(\"You : \")\n",
    "\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Chatbot : Good Bye!\")\n",
    "        break\n",
    "\n",
    "    bot_response, chat_history_ids = chat_with_bot(user_input, chat_history_ids)\n",
    "    print(f\"Chatbot : {bot_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd55be3-6894-4278-bf61-7aff4421c892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.14",
   "language": "python",
   "name": "tf2.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
